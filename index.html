<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Object-Centric Editing of Dynamic 2D Gaussian Scenes</title> <!-- More specific title -->
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Roboto+Mono:wght@400;500&family=Source+Serif+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <!-- MathJax Configuration -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['\\[', '\\]']]
      },
      svg: {
        fontCache: 'global'
      }
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="page-container">
        <nav id="table-of-contents">
            <h3>Table of Contents</h3>
            <ul>
                <li><a href="#introduction">1. Introduction</a></li>
                <li><a href="#background">2. Background</a></li>
                <li><a href="#methodology">3. Our Approach</a>
                    <ul>
                        <li><a href="#baseline-model">3.1 Baseline Model</a></li>
                        <li><a href="#optical-flow-loss">3.2 Optical Flow Loss</a></li>
                    </ul>
                </li>
                <li><a href="#results">4. Experiments & Results</a>
                    <ul>
                        <li><a href="#void-tracking">4.1 Void Tracking Comparison</a></li>
                        <li><a href="#wave-effect">4.2 The 'Wave' Effect: Apparent Motion</a></li>
                        <li><a href="#gaussian-motion-viz">4.3 Gaussian Motion Analysis</a></li>
                        <li><a href="#limitations">4.4 Limitations</a></li>
                        <li><a href="#teaser-demo">4.5 Teaser: Object Recolor & Removal</a></li>
                    </ul>
                </li>
                <li><a href="#discussion">5. Discussion</a></li>
                <li><a href="#conclusion">6. Conclusion</a></li>
                <li><a href="#references">7. References</a></li>
            </ul>
        </nav>

        <main class="main-content-column">
            <header>
                <h1>Flowing Gaussians: Enhancing Object Coherence in Dynamic 2D Scenes</h1>
                <p class="subtitle">Investigating Optical Flow Supervision for Robust Video Editing</p>
                <p class="authors">Your Name(s) Here</p>
                <p class="date">May 13, 2025</p>
            </header>

            <article>
                <section id="introduction">
                    <h2>1. Introduction</h2>
                    <p>The ability to edit video content by manipulating specific objects—removing them, changing their appearance, or altering their motion—is a long-standing goal in computer graphics and vision. Recently, dynamic neural radiance fields, particularly methods like 2D and 3D Gaussian Splatting, have shown remarkable success in representing complex dynamic scenes with high fidelity and fast rendering speeds. These methods model a scene as a collection of explicit primitives (Gaussians) whose properties (position, shape, color, opacity) can change over time.</p>
                    <p>However, a key challenge arises: these primitives are typically optimized solely for pixel-wise reconstruction of the input video. As a result, they often do not learn to group themselves into semantically meaningful, coherently moving objects. Instead, the appearance of a moving object might be reconstructed by a "wave" of different local Gaussians activating at different times, rather than a fixed set of "object Gaussians" rigidly tracking the object. This lack of object-centric coherence severely hinders downstream editing tasks. For instance, if one attempts to remove an object by making the Gaussians that form its appearance in an initial frame transparent, the resulting "void" often remains static or moves incoherently, while the actual object (rendered by other Gaussians at later times) moves away from this void (Fig. X - show static void baseline later).</p>
                    <p>In this project, we investigate methods to improve the object-centric coherence of learned Gaussian trajectories in dynamic 2D scenes. Specifically, we explore the use of pre-computed optical flow as a dense supervisory signal during training to encourage the learned motion of individual Gaussians to align with observed pixel-level motion. We demonstrate that this approach leads to more temporally consistent object representations, significantly improving the quality of editing operations like object removal and recoloring, where the edit more faithfully follows the target object. We also analyze the learned Gaussian behaviors and discuss the remaining challenges, such as long-term tracking limitations.</p>
                </section>

                <section id="background">
                    <h2>2. Background</h2>
                    <p>Provide context on Dynamic 2D Gaussian Splatting, the challenges in dynamic scene editing, the role of optical flow, and perhaps brief mentions of object-centric NeRFs (e.g., DynaVol, D^2NeRF) as related concepts for scene decomposition. Cite relevant papers.</p>
                </section>

                <section id="methodology">
                    <h2>3. Our Approach</h2>
                    <section id="baseline-model">
                        <h3>3.1 Baseline Dynamic 2D Gaussian Model</h3>
                        <p>Describe the core Gaussian parameterization (position \(xy(t)\), Cholesky \(L(t)\), opacity \(\alpha(t)\), static color \(c\)) and how the dynamic parameters are modeled using B-splines. Mention the reconstruction loss \(L_{\text{recon}}\).</p>
                        <p>Equation for B-spline trajectory: \(p(t) = \sum N_{i,p}(t) \cdot CP_i\)</p>
                        <p>Equation for reconstruction loss: \(L_{\text{recon}} = \| \text{Render}(G, t) - I_{\text{gt}}(t) \|^2\)</p>
                    </section>
                    <section id="optical-flow-loss">
                        <h3>3.2 Optical Flow Supervision</h3>
                        <p>Explain the "wave effect" and the problem of local oscillations with the baseline. Introduce the optical flow loss: how flow is pre-computed (e.g., Farneback/RAFT), how model-implied flow is derived, and the loss formulation \(L_{\text{flow}}\). Mention the opacity thresholding for applying the loss.</p>
                        <p>Equation for model-implied flow: \(v_{\text{model},i}(t) = xy_i(t+dt) - xy_i(t)\)</p>
                        <p>Equation for optical flow loss: \(L_{\text{flow}} = \sum_t \sum_i w_i \cdot \|v_{\text{model},i}(t) - v_{\text{gt},i}(t)\|^2\)</p>
                        <p>Total Loss: \(L_{\text{total}} = L_{\text{recon}} + \lambda_{\text{flow}} \cdot L_{\text{flow}}\)</p>
                    <figure>
                            <img src="images/flow_loss_diagram.png" alt="Optical Flow Loss Diagram" style="max-width: 80%; height: auto;">
                            <figcaption>Fig. X: Conceptual diagram of applying optical flow loss to Gaussian motion.</figcaption>
                    </figure>
                    </section>
                </section>

                <section id="results">
                    <h2>4. Experiments & Results</h2>
                    <p>Detail your experimental setup: datasets (boba, bear, etc.), model parameters, training details, and evaluation metrics (qualitative focus, PSNR/SSIM for reconstruction).</p>

                    <section id="void-tracking">
                        <h3>4.1 Void Tracking: Baseline vs. Optical Flow</h3>
                        <p>A critical test for object removal is how well the "void" (the transparent region) tracks the removed object. The baseline model often exhibits a static void, while our optical flow supervised model shows improved tracking.</p>
                        <div class="comparison-container">
                            <h4>Boba Cup Removal: Void Tracking</h4>
                             <div class="video-wrapper">
                                <div class="juxtapose" data-startingposition="50%" data-showlabels="true" data-label="Slide">
                                    <img src="images/boba_void_baseline_frame.jpg" data-label="Baseline (Static Void)">
                                    <img src="images/boba_void_flow_frame.jpg" data-label="Ours (Tracking Void)">
                                </div>
                                <video autoplay loop muted playsinline width="100%" class="comparison-video-under">
                                    <source src="videos/downstream/boba_remove_baseline.mp4" type="video/mp4">
                                </video>
                                <video autoplay loop muted playsinline width="100%" class="comparison-video-over">
                                    <source src="videos/downstream/boba_remove_optical_flow.mp4" type="video/mp4">
                                </video>
                                <div class="comparison-slider"></div>
                            </div>
                            <figcaption>Fig. X: Void Tracking Comparison: Baseline (left) vs. Optical Flow Model (right).</figcaption>
                        </div>
                    </section>

                    <section id="wave-effect">
                        <h2>4.2 The 'Wave' Effect: Apparent Motion from Local Gaussian Activity</h2>
                        <p>A common pitfall in optimizing dynamic explicit representations like Gaussian splatting solely for reconstruction is the emergence of what we term the "wave effect." Instead of a consistent set of Gaussians tracking an object, the appearance of motion can be achieved by different, spatially localized Gaussians activating and deactivating over time, much like a stadium wave. This creates an illusion of movement without true object coherence, hindering editing tasks. The following animation illustrates this concept metaphorically.</p>
                    <figure>
                            <img src="videos/fallacy/Lwave-Red-2.gif" alt="Wave effect metaphor: air molecules creating a wave" style="max-width: 60%; height: auto; border: 1px solid var(--border-color);">
                            <figcaption>Fig. X: Metaphorical illustration of the 'wave effect'. Local elements (like air molecules or Gaussians) oscillate or activate sequentially, creating a propagating wave pattern, distinct from the collective, rigid movement of a defined object.</figcaption>
                    </figure>

                        <p>We observe this phenomenon in our baseline model for the <code>boba_69</code> experiment. The videos below show Gaussian centers and their rendered output when trained without explicit motion guidance.</p>
                    <figure>
                            <div class="video-row video-row-layout-2">
                                <div class="video-item">
                                    <p class="video-label">Gaussian Centers (Baseline)</p>
                                    <video autoplay loop muted playsinline>
                                        <source src="videos/fallacy/boba_69_centers_iter_10000_fast.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                                <div class="video-item">
                                    <p class="video-label">Rendered Video (Baseline)</p>
                                    <video autoplay loop muted playsinline>
                                        <source src="videos/fallacy/boba_69_rendered_original_only_iter_10000_fast.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                        </video>
                                </div>
                            </div>
                            <figcaption>Fig. Y: Baseline <code>boba_69</code> scene: Gaussian centers (left) and corresponding rendered output (right). Note the local activity rather than cohesive object motion.</figcaption>
                    </figure>

                        <p>Further diagnostic experiments highlight this. When Gaussians are initialized only in one region (e.g., the right half of the scene), they tend to remain localized or exhibit oscillatory behavior to reconstruct appearances, rather than translating coherently with perceived objects. This ultimately impacts tasks like object removal, where the "void" left by a removed object may not track the object's true path.</p>
                        <figure>
                            <div class="video-row video-row-layout-3">
                                <div class="video-item">
                                    <p class="video-label">Diag: Centers (Right Init)</p>
                                    <video autoplay loop muted playsinline>
                                        <source src="videos/fallacy/boba_69_diag_starts_right_centersb.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                                 <div class="video-item">
                                    <p class="video-label">Diag: Rendered (Right Init)</p>
                                    <video autoplay loop muted playsinline>
                                        <source src="videos/fallacy/boba_69_diag_starts_right_renderedb.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                                <div class="video-item">
                                    <p class="video-label">Removal Attempt (Baseline)</p>
                                    <video autoplay loop muted playsinline>
                                        <source src="videos/fallacy/boba_69_remove_iter10000b.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                            </div>
                            <figcaption>Fig. Z: Diagnostic visualizations for <code>boba_69</code>: Gaussian centers after right-half initialization (left), corresponding render (middle), and an object removal attempt showing limited tracking (right).</figcaption>
                        </figure>
                    </section>

                    <section id="gaussian-motion-viz">
                        <h3>4.3 Gaussian Motion Analysis</h3>
                        <p>To understand *why* optical flow helps, we visualize the motion of Gaussians. The baseline model often shows Gaussians with local, oscillatory movements (the "wave effect"), while the flow-supervised model encourages more coherent, long-range transport.</p>
                        <figure class="side-by-side-video-pair">
                            <div class="video-row">
                                <video loop muted playsinline width="48%" style="display: inline-block;" class="video-left">
                                    <source src="videos/optical_flow/bear_39_centers_iter_10000_optflow0.mp4" type="video/mp4">
                                </video>
                                <video loop muted playsinline width="48%" style="display: inline-block;" class="video-right">
                                    <source src="videos/optical_flow/bear_39_centers_iter_10000_optflow10000.mp4" type="video/mp4">
                                </video>
                            </div>
                            <!-- <div class="sync-controls" style="text-align: center; margin-top: 10px;">
                                <button class="play-pause-pair-btn">Play Bear Comparison</button>
                            </div> -->
                            <figcaption>Fig. X: Gaussian center tracks for Bear scene. Left: Baseline (No Flow Loss). Right: With Optical Flow Loss.</figcaption>
                        </figure>
                        <figure class="side-by-side-video-pair boba-video-pair">
                            <div class="video-row">
                                <video loop muted playsinline width="48%" style="display: inline-block;" class="video-left">
                                    <source src="videos/optical_flow/boba_40_centers_iter_10000_optflow0.mp4" type="video/mp4">
                                </video>
                                <video loop muted playsinline width="48%" style="display: inline-block;" class="video-right">
                                    <source src="videos/optical_flow/boba_40_centers_iter_10000_optflow100000.mp4" type="video/mp4">
                            </video>
                        </div>
                            <!-- <div class="sync-controls" style="text-align: center; margin-top: 10px;">
                                <button class="play-pause-pair-btn">Play Boba Comparison</button>
                            </div> -->
                            <figcaption>Fig. Y: Gaussian center tracks for Boba scene. Left: Baseline (No Flow Loss). Right: With Optical Flow Loss.</figcaption>
                        </figure>
                    </section>

                    <section id="limitations">
                        <h3>4.4 Limitations & Failure Cases</h3>
                        <p>While optical flow supervision improves consistency, challenges remain, especially for very long sequences or extremely complex motions where tracking can degrade after many frames.</p>
                        <figure>
                             <video controls loop muted playsinline width="70%">
                                <source src="videos/downstream/long_sequence_breakdown.mp4" type="video/mp4">
                            </video>
                            <figcaption>Fig. X: Example of tracking degradation over a longer sequence (e.g., after 50 frames).</figcaption>
                        </figure>
                    </section>

                    <section id="teaser-demo">
                        <h3>4.5 Teaser: Object Recolor and Removal</h3>
                        <p>The primary capability we aim for is robust object editing. Here we showcase our method's ability to recolor and remove a target object (a bear) with improved temporal consistency compared to a baseline model trained without optical flow guidance.</p>

                        <!-- Interactive Slider for Bear Recolor -->
                        <div class="comparison-container">
                            <h4>Bear Recolor Comparison</h4>
                            <div class="video-wrapper">
                                <video autoplay loop muted playsinline width="100%" class="comparison-video-under">
                                    <source src="videos/downstream/bear_39_bear interior_recolored_to_0_0_0_0_1_0_w0_20_iter10000_optflow30000_pts249243b.mp4" type="video/mp4">
                                </video>
                                <video autoplay loop muted playsinline width="100%" class="comparison-video-over">
                                    <source src="videos/downstream/bear_39.mp4" type="video/mp4">
                                </video>
                                <div class="comparison-slider"></div>
                        </div>
                            <figcaption>Fig. X: Interactive Video Slider: Original Bear (left) vs. Recolored Bear (right). (Drag the slider!)</figcaption>
                        </div>
                    </section>
                </section>

                <section id="discussion">
                    <h2>5. Discussion</h2>
                    <p>Elaborate on the "wave effect" theory. Why does optical flow help? What are the trade-offs (e.g., sensitivity to flow quality)? How does this compare to other object-centric approaches? </p>
                </section>

                <section id="conclusion">
                    <h2>6. Conclusion</h2>
                    <p>Summarize your key findings: optical flow as a supervisory signal is a promising way to enhance object coherence in dynamic 2D Gaussian splatting, leading to better editing results. Acknowledge limitations and point to future work.</p>
                </section>

                <section id="references">
                    <h2>7. References</h2>
                    <ul>
                        <li>[1] Kerbl, B., et al. (2023). 3D Gaussian Splatting...</li>
                        <li>[2] Kirillov, A., et al. (2023). Segment Anything.</li>
                        <li>[3] Teed, Z., & Deng, J. (2020). RAFT: Recurrent All-Pairs Field Transforms for Optical Flow.</li>
                        <li>(Add other relevant citations, e.g., for your base dynamic 2D GS method, D^2NeRF, DynaVol if discussed)</li>
                    </ul>
                </section>

                <footer>
                    <p>© 2025 Your Name(s). Project for 6.8300 Computer Vision. Optional: <a href="your_github_repo_link">GitHub Repository</a>.</p>
                </footer>
            </article>
        </main>
    </div>
    <!-- JuxtaposeJS for image comparison slider (optional, can be replaced with custom video slider) -->
    <link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css">
    <script src="https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js"></script>
    <script src="script.js"></script>
</body>
</html>
