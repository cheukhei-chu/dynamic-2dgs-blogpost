<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Flowing Gaussians: Object Editing in Dynamic 2D Scenes</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Roboto+Mono:wght@400;500&family=Source+Serif+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <script>
    MathJax = { tex: { inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']] }, svg: { fontCache: 'global' } };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css">
</head>
<body>
    <div class="page-container">
        <nav id="table-of-contents">
            <h3>Table of Contents</h3>
            <ul>
              <li><a href="#introduction">1. Introduction</a>
                <ul>
                  <li><a href="#motivation">1.1 Motivation</a></li>
                  <li><a href="#overview">1.2 Overview</a></li>
                  <li><a href="#literature-review">1.3 Related Work</a></li>
                </ul>
              </li>

              <li><a href="#problem-statement">2. Problem Statement & Challenges</a>
                <ul>
                  <li><a href="#dynamic-2dgs">2.1 Dynamic 2D Gaussian Splatting</a></li>
                  <li><a href="#challenge-coherence">2.2 The Coherence Challenge</a></li>
                </ul>
              </li>

              <li><a href="#our-approach">3. Our Approach: Optical Flow Supervision</a>
                <ul>
                  <li><a href="#baseline-model-details">3.1 Baseline Dynamic 2D Gaussian Model</a></li>
                  <li><a href="#trajectory-comparison">3.2 Trajectory Models: Polynomials vs. B-Splines</a></li>
                  <li><a href="#wave-effect-analysis-detailed">3.3 Deeper Dive: The "Wave Effect"</a></li>
                  <li><a href="#optical-flow-integration">3.4 Integrating Optical Flow Loss</a></li>
                </ul>
              </li>

              <li><a href="#experiments-results">4. Experiments & Results</a>
                <ul>
                  <li><a href="#setup">4.1 Experimental Setup</a></li>
                  <li><a href="#reconstruction-quality">4.2 Reconstruction Quality</a></li>
                  <li><a href="#gaussian-motion-analysis">4.3 Gaussian Motion Analysis</a></li>
                  <li><a href="#editing-demos">4.4 Editing Demonstrations</a></li>
                  <li><a href="#temporal-interpolation">4.5 Temporal Interpolation with Optical Flow</a></li>
                  <li><a href="#limitations-failures">4.6 Limitations & Failure Cases</a></li>
                </ul>
              </li>

              <li><a href="#discussion">5. Discussion</a></li>
              <li><a href="#conclusion">6. Conclusion & Future Work</a></li>
              <li><a href="#references">7. References</a></li>
            </ul>
          </nav>

        <main class="main-content-column">
            <header>
                <h1>Flowing Gaussians: Optical Flow Guided Dynamic 2D Gaussian Splatting for Efficient Video Representation</h1>
                <p class="subtitle">An Investigation into Improving Temporal Coherence of Dynamic 2D Gaussian Splatting Representations</p>
                <p class="authors">Cheuk Hei Chu, Margulan Ismoldayev</p>
                <p class="date">May 13, 2025</p>
                <p class="repo-links" style="text-align: center; margin-top: 10px; font-family: var(--font-sans);">
                    <a href="https://github.com/ismoldayev/dynamic-2dgs" target="_blank" rel="noopener noreferrer">[Code Repository]</a>
                </p>
            </header>

            <div class="fun-examples-row" style="text-align: center; margin-bottom: 20px;">
              <div style="display: flex; justify-content: space-around; align-items: flex-start; flex-wrap: wrap;">
                <div class="video-item" style="margin: 10px; flex: 1; min-width: 280px; max-width: 30%;">
                  <video autoplay loop muted playsinline controlslist="nodownload" style="width: 100%; height: auto; border: 1px solid var(--border-color);">
                    <source src="videos/car_60_car_recolored_to_1_0_0_0_0_0_w0_60_iter10000_optflow100000_pts249244.MP4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <div class="video-item" style="margin: 10px; flex: 1; min-width: 280px; max-width: 30%;">
                  <video autoplay loop muted playsinline controlslist="nodownload" style="width: 100%; height: auto; border: 1px solid var(--border-color);">
                    <source src="videos/car_60_diag_rendered_car.MP4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <div class="video-item" style="margin: 10px; flex: 1; min-width: 280px; max-width: 30%;">
                  <video autoplay loop muted playsinline controlslist="nodownload" style="width: 100%; height: auto; border: 1px solid var(--border-color);">
                    <source src="videos/bear_82_bear_recolored_to_1_0_1_0_0_0_w0_20_iter10000_optflow100000_pts249244.MP4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </div>

            <article>
                <section id="introduction">
                    <h2>1. Introduction</h2>

                    <!-- 1.1 -------------------------------------------------------- -->
                    <section id="motivation">
                      <h3>1.1 Motivation</h3>
                   <p>Recent advances in neural rendering, particularly 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023), have enabled high-fidelity, real-time rendering of static scenes. The extension to Dynamic 3D Gaussians (Luiten et al., 2023) further allows for the modeling of complex dynamic 3D scenes, capturing motion and enabling novel-view synthesis over time along with dense 6-DOF tracking. While powerful, these methods explicitly model the full 3D scene structure, which can be computationally intensive and potentially overkill for applications focused solely on analyzing or manipulating video content from the original viewpoints.</p>
                   <p>Concurrently, the efficiency of using 2D Gaussians for static image representation and compression has been demonstrated by GaussianImage (Zhang et al., 2024), which achieves extremely fast rendering speeds by representing images as collections of optimized 2D Gaussian primitives. This motivates the question of whether dynamic 2D Gaussian representation can effectively model video sequences, capturing complex motion and temporal coherence, and as such enabling basic video analysis while offering significant efficiency advantages over 3D dynamic modeling. In this project, we propose Flowing Gaussians, a method of representing a video using dynamic 2D Gaussians, supervised by optical flow, that allows all sorts of efficient object-based downstream video editing tasks. </p>
               </section>

                    <!-- 1.2 -------------------------------------------------------- -->
                    <section id="overview">
                      <h3>1.2 Overview</h3>
                      <p>Dynamic 2D Gaussian Splatting has emerged as a promising technique for representing videos, offering high-fidelity reconstruction and fast rendering. However, standard training procedures often result in representations where individual Gaussians lack long-term object-centric coherence. This fundamentally limits the ability to perform robust object-level editing tasks like removal or recoloring, as edits applied to Gaussians identified at one point in time do not consistently follow the object's perceived motion.</p>
                   <p>Our initial explorations revealed an intriguing phenomenon: the appearance of moving objects is often formed by what we term a "wave effect," where different local Gaussians activate and contribute at different times, rather than a consistent set of "object Gaussians" rigidly tracking the object. This project investigates the hypothesis that supervising the training of dynamic 2D Gaussians with dense optical flow information can enforce more coherent, object-like trajectories for the learned primitives. We demonstrate that by incorporating an optical flow consistency loss, the resulting model exhibits significantly improved temporal tracking of edited objects, allowing for more plausible object removal and recoloring. We analyze the change in learned Gaussian behavior and discuss the implications and remaining challenges of this approach.</p>
                   <p>At the heart of Flowing Gaussians are two key design decisions aimed at enhancing both the representational power and training efficiency for dynamic scenes. Firstly, diverging from simpler trajectory models such as in VeGaS (Smolak-Dyżewska et al., 2024), we leverage B-splines to model the temporal evolution of individual Gaussian parameters (such as position and shape). Secondly, to ensure these dynamic Gaussians accurately track objects through complex movements and interactions, we integrate optical flow as an explicit guiding signal during training. These decisions will be explained in depth and illustrated using video examples in order in Sections 3 and 4.</p>
                   <p>In this post, we will detail the architecture of our optical flow-guided dynamic 2D Gaussian Splatting method, elaborating on design decisions, and demonstrating its efficacy in video reconstruction and object tracking in Sections 3 and 4. Finally, we will showcase its application in downstream video editing tasks, highlighting the efficiency gained by our one-shot segmentation approach for persistent object editing.</p>
               </section>

                    <!-- 1.3 -------------------------------------------------------- -->
                    <section id="literature-review">
                      <h3>1.3 Related Work</h3>
                      <p>Our work builds upon several related previous works:</p>
                      <ul>
                        <li><strong>3D Gaussian Splatting (3DGS):</strong> The foundational work by Kerbl et al. (Kerbl et al., 2023) introduced efficient static scene rendering using 3D Gaussians.</li>
                        <li><strong>Dynamic 3D Gaussians:</strong> Luiten et al. (Luiten et al., 2023) extended 3DGS to dynamic scenes by allowing Gaussian parameters (position, rotation) to vary over time, enforced by physical priors (local rigidity), achieving state-of-the-art dynamic novel-view synthesis and tracking. However, its reliance on multi-view data and explicit 3D reasoning makes it complex.</li>
                        <li><strong>Static 2D Gaussian Representation:</strong> Zhang et al. (Zhang et al., 2024) (GaussianImage) demonstrated the use of optimized 2D Gaussians for compact static image representation and ultra-fast rendering, using an efficient accumulated blending technique suitable for 2D. This work lacks any temporal dynamics.</li>
                        <li><strong>Other Dynamic Scene Representations:</strong> Methods like Dynamic NeRFs (e.g., D-NeRF, Nerfies) often model dynamics via deformation fields applied to a canonical space or use time-dependent coordinate inputs, representing alternative approaches to modeling video.</li>
                      </ul>
                    </section>
                  </section>

                <section id="problem-statement">
                    <h2>2. Problem Statement & Challenges</h2>



                    <section id="dynamic-2dgs">
                        <h3>2.1 Dynamic 2D Gaussian Splatting</h3>
                        <p>Our work builds upon the concept of 2D Gaussian Splatting, which represents visual information using a collection of 2D anisotropic Gaussian primitives. Each Gaussian \(G_i\) is defined by its 2D center position \(xy_i\), a 2D covariance matrix \(\Sigma_i\) (controlling its shape and orientation), an opacity \(\alpha_i\), and a color \(c_i\). For dynamic scenes (videos), these parameters, particularly position, covariance, and opacity, become functions of time \(t\): \(xy_i(t), \Sigma_i(t), \alpha_i(t)\). In our implementation, similar to recent dynamic Gaussian approaches, we model these time-varying parameters using B-splines. For instance, the x-coordinate of a Gaussian's center is given by \(x_i(t) = \sum_{k=0}^{K-1} N_{k,p}(t) \cdot CP_{i,x,k}\), where \(CP_{i,x,k}\) are learned control points and \(N_{k,p}(t)\) are B-spline basis functions of degree \(p\). The color \(c_i\) is typically kept static per Gaussian. Rendering a frame at time \(t\) involves projecting all active Gaussians onto the image plane and alpha-compositing their contributions. This explicit, differentiable representation allows for fast rendering and optimization via gradient descent.</p>
                    </section>
                    <section id="challenge-coherence">
                        <h3>2.2 The Coherence Challenge</h3>
                            <p>Initial analysis of Gaussian primitives trained solely with a reconstruction objective reveals characteristic motion patterns. The animations below (Figure 1) depict the trajectories of Gaussian centers for two distinct dynamic scenes: <code>boba_69</code> (a boba tea cup moving across a surface) and <code>margulan_63</code> (a person ascending stairs). While the rendered output of these baseline models achieves high fidelity in frame reconstruction (cf. Section 4.2), the underlying motion of individual Gaussian centers displays a notable phenomenon.</p>
                            <figure class="side-by-side-video-pair">
                                <div class="video-row video-row-layout-2">
                                    <div class="video-item">
                                        <p class="video-label">Boba Cup Centers (Baseline)</p>
                                        <video autoplay loop muted playsinline class="vertical-video-constrained">
                                            <source src="videos/fallacy/boba_69_centers_iter_10000_fast.mp4" type="video/mp4">
                                        </video>
                                    </div>
                                     <div class="video-item">
                                        <p class="video-label">"Walking Up Stairs" Centers (Baseline)</p>
                                        <video autoplay loop muted playsinline class="vertical-video-constrained">
                                            <source src="videos/fun_results/margulan_63_centers_iter_10000.MP4" type="video/mp4">
                                        </video>
                                    </div>
                                </div>
                                <figcaption>Fig 1: Animated Gaussian centers from baseline models (no optical flow loss). Left: "boba_69" scene. Right: "margulan_63" scene (person walking up stairs). Notice the localized, often oscillatory, motion of individual Gaussians creating a collective "wave" that forms the appearance of moving objects.</figcaption>
                            </figure>
                            <p>This observed behavior, which we will call the 'wave effect,' describes how apparent object motion is often synthesized through the sequential activation of spatially localized or oscillating primitives, rather than the coherent, collective transport of a stable group of Gaussians. Such a mechanism presents a significant challenge for achieving robust object-level editing.</p>

                        <p>A primary challenge when training such models solely with a pixel-wise reconstruction loss (e.g., L1 + MS-SSIM) is that the learned Gaussians often lack object-centric coherence. The optimization prioritizes matching the target frame at each instant, which can be achieved even if individual Gaussians do not consistently track semantic objects. Instead, a "wave effect" can emerge: the appearance of a moving object is formed by different, spatially localized Gaussians activating, deactivating, or subtly shifting over time, much like a stadium wave or the propagation of sound. This creates a convincing illusion of motion at the macroscopic level, but the underlying primitives may not possess persistent object identity or long-range, coherent trajectories.</p>
                        <figure>
                            <img src="videos/fallacy/Lwave-Red-2.gif" alt="Wave effect metaphor" style="max-width: 50%; height: auto; border: 1px solid var(--border-color);">
                            <figcaption>Fig 2: The "wave effect" analogy. Local elements (representing Gaussians) create an illusion of a propagating wave (apparent object motion) through sequential local activities, without any single element traversing the entire path.</figcaption>
                        </figure>
                        <p>This lack of true object tracking is demonstrated in Figure 3 using our baseline model on the <code>boba_69</code> video.</p>
                         <figure>
                            <video autoplay loop muted playsinline width="70%" class="vertical-video-constrained standalone-boba-video">
                                <source src="videos/fallacy/boba_69_remove_iter10000b.mp4" type="video/mp4">
                            </video>
                            <figcaption>Fig 3: Object removal attempt using the baseline model ('boba_69').</figcaption>
                        </figure>
                    </section>
                </section>

                <section id="our-approach">
                    <h2>3. Our Approach: Optical Flow Supervision</h2>
                    <section id="baseline-model-details">
                        <h3>3.1 Baseline Dynamic 2D Gaussian Model</h3>
                        <p>Our baseline model represents a video using \(N\) 2D Gaussian primitives. Each Gaussian \(G_i\) is characterized by a static color \( \mathbf{c}_i \in \mathbb{R}^3 \) and time-varying parameters for its 2D center \( \mathbf{xy}_i(t) \in \mathbb{R}^2 \), its 2D covariance matrix \( \mathbf{\Sigma}_i(t) \), and its opacity \( \alpha_i(t) \in [0,1] \). The covariance \( \mathbf{\Sigma}_i(t) = \mathbf{L}_i(t)\mathbf{L}_i(t)^T \) is parameterized by the elements of its lower-triangular Cholesky factor \( \mathbf{L}_i(t) = [[L_{00}(t), 0], [L_{10}(t), L_{11}(t)]] \). These dynamic parameters (centers, Cholesky elements, opacity) are modeled as B-spline functions of time \(t \in [0,1]\) with degree \(p\) and \(K\) control points \(\{\mathbf{CP}_{i,j}\}\):
                        </p>
                        <p>\[ \text{param}_i(t) = \sum_{j=0}^{K-1} N_{j,p}(t) \cdot \mathbf{CP}_{i,j}^{\text{param}} \]</p>
                        <p>The model is trained by minimizing an L2 reconstruction loss between the rendered frames \(\hat{I}(t)\) and ground truth frames \(I_{\text{gt}}(t)\):</p>
                        <p>\[ \mathcal{L}_{\text{recon}} = \sum_{t} \|\hat{I}(t) - I_{\text{gt}}(t)\|_2^2 \]</p>
                    </section>
                    <section id="trajectory-comparison">
                        <h3>3.2 Trajectory Models: Polynomials vs. B-Splines</h3>
                        <p>The choice of how to model the temporal evolution of Gaussian parameters is crucial. While high-degree polynomials can represent complex paths, they can also be prone to instability or require many parameters to capture intricate motions faithfully. B-splines, with their local control property, offer a more flexible and often more robust alternative for modeling smooth trajectories. For a comparable number of learnable parameters (e.g., polynomial degree vs. number of B-spline control points), B-splines typically yield better reconstruction quality and more natural-looking motion, as demonstrated in the comparison below for the <code>boba_69</code> scene. The polynomial model (left) may exhibit more erratic or less smooth paths compared to the B-spline model (right).</p>
                        <div style="max-width: 500px; margin: 2.5rem auto;">
                            <figure class="side-by-side-video-pair">
                                <div class="video-row video-row-layout-2">
                                    <div class="video-item">
                                        <p class="video-label">Polynomial (Degree 30)</p>
                                        <video autoplay loop muted playsinline>
                                            <source src="videos/bspline/boba_69_polydeg30b.mp4" type="video/mp4">
                                            Your browser does not support the video tag.
                                        </video>
                                    </div>
                                    <div class="video-item">
                                        <p class="video-label">B-Spline (30 Control Points)</p>
                                        <video autoplay loop muted playsinline>
                                            <source src="videos/bspline/boba_69_bsplinectrl30b.mp4" type="video/mp4">
                                            Your browser does not support the video tag.
                                        </video>
                                    </div>
                                </div>
                                <figcaption>Fig 4: Trajectory Model Comparison (Boba scene). Left: Polynomial (degree 30). Right: B-Spline (30 control points).</figcaption>
                            </figure>
                        </div>
                    </section>
                    <section id="wave-effect-analysis-detailed">
                        <h3>3.3 Deeper Dive: The "Wave Effect"</h3>
                        <p>To understand the baseline's limitations in learning coherent object motion, we conducted diagnostic experiments. One such experiment involved rendering only those Gaussians whose initial positions (\(t=0\)) were on the right half of the screen for the <code>boba_69</code> video (where the boba cup moves from right to left). As seen in Figure 5, these Gaussians largely exhibit local, oscillatory motion and fail to translate coherently with the boba cup across the frame. This supports our "wave effect" hypothesis: apparent motion is achieved by local activations rather than consistent transport of object-specific primitives.</p>
                        <div style="max-width: 500px; margin: 2.5rem auto;">
                            <figure class="side-by-side-video-pair">
                                <div class="video-row video-row-layout-2">
                                    <div class="video-item">
                                        <p class="video-label">Centers (Right Init, Baseline)</p>
                                        <video autoplay loop muted playsinline>
                                            <source src="videos/fallacy/boba_69_diag_starts_right_centersb.mp4" type="video/mp4">
                                        </video>
                                    </div>
                                     <div class="video-item">
                                        <p class="video-label">Rendered (Right Init, Baseline)</p>
                                        <video autoplay loop muted playsinline>
                                            <source src="videos/fallacy/boba_69_diag_starts_right_renderedb.mp4" type="video/mp4">
                                        </video>
                                    </div>
                                </div>
                                <figcaption>Fig 5: Diagnostic for <code>boba_69</code> baseline (Gaussians initialized on right half)</figcaption>
                            </figure>
                        </div>
                    </section>
                    <section id="optical-flow-integration">
                        <h3>3.4 Integrating Optical Flow Loss</h3>
                        <p>To encourage Gaussians to learn trajectories that better reflect true object motion, we introduce an optical flow consistency loss. First, we pre-compute dense optical flow \( \mathbf{f}_{\text{gt}}(u,v,t \to t+1) \) between all consecutive ground truth frames \(I_{\text{gt}}(t)\) and \(I_{\text{gt}}(t+1)\) using OpenCV's Farneback algorithm. </p>
                    <p>For each Gaussian \(i\) with opacity \(\alpha_i(t)\) above a threshold \(\tau_{\text{opac}}\) (e.g., 0.2) at time \(t\), its learned B-spline trajectory implies a 2D displacement from frame \(t\) to \(t+dt\). Its position at time \(t\) is \(xy_i(t)\), and its model-predicted position at the next frame time \(t'\) is \(xy_i(t')_{\text{model}}\). The model-implied displacement vector is:</p>
                    <p>\[ \Delta \mathbf{xy}_{\text{model},i}(t) = xy_i(t')_{\text{model}} - xy_i(t) \]</p>
                    <p>We sample the pre-computed ground truth optical flow \(\Delta \mathbf{xy}_{\text{gt},i}(t)\) at the Gaussian's projected center \(xy_i(t)\). Both displacements are converted to normalized coordinate space. The optical flow loss penalizes the squared L2 norm of their difference:</p>
                    <p>\[ \mathcal{L}_{\text{flow}} = \sum_{t} \sum_{i \text{ s.t. } \alpha_i(t) > \tau_{\text{opac}}} \left\| \Delta \mathbf{xy}_{\text{model},i}(t) - \Delta \mathbf{xy}_{\text{gt},i}(t) \right\|^2_2 \]</p>
                    <p>The total training loss combines reconstruction and flow consistency:</p>
                    <p>\[ \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{recon}} + \lambda_{\text{flow}} \cdot \mathcal{L}_{\text{flow}} \]</p>
                    <p>The hyperparameter \(\lambda_{\text{flow}}\) (e.g., set to \(1 \times 10^4\) in our experiments) balances reconstruction fidelity with motion consistency.</p>
                    </section>
                </section>

                <section id="experiments-results">
                    <h2>4. Experiments & Results</h2>
                    <section id="setup">
                        <h3>4.1 Experimental Setup</h3>
                        <p>We evaluate our approach on several short video clips, including <code>boba_69</code> (a boba tea cup moving across a textured background, 69 frames, 192x336 resolution after processing) and <code>bear_39</code> (a bear walking, 39 frames). Our models typically use 40,000 2D Gaussians, with B-spline trajectories (K=20 control points for position and Cholesky elements, degree \(p=3\)). Opacity is modeled with a polynomial of degree 0 (static per Gaussian after initialization). Training is performed for 10,000 iterations using the Adan optimizer (learning rate 1e-3) and the L2 reconstruction loss. For the optical flow supervised model, \(\lambda_{\text{flow}}\) was set to \(1 \times 10^4\). Object segmentation for editing is performed post-training using SAM on user-annotated keyframes, followed by optional motion coherence filtering (DBSCAN on B-spline control point features).</p>
                    </section>
                    <section id="reconstruction-quality">
                        <h3>4.2 Reconstruction Quality</h3>
                        <p>Adding the optical flow loss acts as a regularizer and does not significantly degrade, and in some cases slightly improves, final reconstruction quality compared to the baseline. </p>
                        <figure>
                            <img src="videos/psnr.png" alt="PSNR Comparison Table" style="max-width: 60%; height: auto; border: 1px solid var(--border-color);">
                            <figcaption>Table 1: Reconstruction quality (PSNR dB).</figcaption>
                        </figure>
                    </section>

                    <section id="gaussian-motion-analysis">
                        <h3>4.3 Gaussian Motion Analysis</h3>
                        <p>Visualizing Gaussian center trajectories clearly illustrates the impact of optical flow. In the baseline, Gaussians exhibit localized, oscillatory motion. With flow supervision, they learn more coherent, long-range transport paths that better align with object movement.</p>
                        <figure class="side-by-side-video-pair">
                            <div class="video-row video-row-layout-2">
                                <div class="video-item"><p class="video-label">Baseline (No Flow)</p><video autoplay loop muted playsinline><source src="videos/optical_flow/bear_39_centers_iter_10000_optflow0.mp4" type="video/mp4"></video></div>
                                <div class="video-item"><p class="video-label">With Optical Flow</p><video autoplay loop muted playsinline><source src="videos/optical_flow/bear_39_centers_iter_10000_optflow10000.mp4" type="video/mp4"></video></div>
                            </div>
                            <figcaption>Fig 6: Gaussian center tracks for "bear_39". Left: Baseline. Right: With Flow Loss.</figcaption>
                        </figure>
                        <div style="max-width: 500px; margin: 2.5rem auto;">
                            <figure class="side-by-side-video-pair">
                                <div class="video-row video-row-layout-2">
                                     <div class="video-item"><p class="video-label">Baseline (No Flow)</p><video autoplay loop muted playsinline><source src="videos/optical_flow/boba_40_centers_iter_10000_optflow0.mp4" type="video/mp4"></video></div>
                                    <div class="video-item"><p class="video-label">With Optical Flow</p><video autoplay loop muted playsinline><source src="videos/optical_flow/boba_40_centers_iter_10000_optflow100000bruh.mp4" type="video/mp4"></video></div>
                                </div>
                                <figcaption>Fig 7: Gaussian center tracks for "boba". Left: Baseline. Right: With Flow Loss.</figcaption>
                            </figure>
                        </div>
                    </section>

                    <section id="editing-demos">
                        <h3>4.4 Editing Demonstrations</h3>
                        <p>The improved object coherence from optical flow supervision enables more compelling and efficient editing applications. To achieve edits like the object recoloring showcased below for the <code>bear_39</code> video, we first apply the Segment Anything Model (SAM) over a user-defined bounding box containing the bear in a reference frame. SAM segments the bear, and we then identify the set of 2D Gaussians whose centers fall within this segmentation mask at that frame. This effectively filters the Gaussians representing the bear. Once these object-specific Gaussians are identified, their parameters (e.g., color) can be modified. Subsequent rendering of the edited video, now with the modified Gaussians, is extremely fast, taking less than 1 second for the entire clip, as it leverages the pre-optimized dynamic representation. The edit consistently follows the bear's motion due to the enhanced trajectory learning from the optical flow supervision.</p>
                        <div class="comparison-container">
                            <div class="video-wrapper">
                                <video autoplay loop muted playsinline width="100%" class="comparison-video-under" id="bearPurpleColor">
                                    <source src="videos/downstream/bear_39_bear_interior_recolored_to_0_0_0_0_1_0_w0_20_iter10000_optflow30000_pts249243b.mp4" type="video/mp4">
                                </video>
                                <video autoplay loop muted playsinline width="100%" class="comparison-video-over" id="bearOriginalColor">
                                    <source src="videos/downstream/bear_39.mp4" type="video/mp4">
                                </video>
                                <div class="comparison-slider" data-for-container=".comparison-container:has(#bearOriginalColor)"></div>
                            </div>
                        </div>
                        <figcaption style="text-align: center; margin-top: 0.75em; font-family: var(--font-sans); font-size: 0.9rem; color: #666; line-height: 1.4;">Fig 8: Interactive Comparison: Original bear (revealed left) vs. Bear recolored to purple (revealed right). Drag slider to compare.</figcaption>
                        <figure>
                            <div class="video-row video-row-layout-2">
                                <div class="video-item">
                                    <p class="video-label">Segmented Bear Rendered</p><video autoplay loop muted playsinline><source src="videos/downstream/bear_39_diag_rendered_bear_interiorb.mp4" type="video/mp4"></video>
                                </div>
                                <div class="video-item">
                                    <p class="video-label">Segmented Bear Centers</p><video autoplay loop muted playsinline><source src="videos/downstream/bear_39_diag_centers_bearb.mp4" type="video/mp4"></video>
                                </div>
                            </div>
                            <figcaption>Fig 9: Visualizing "bear interior" Gaussians from flow-trained model.</figcaption>
                        </figure>
                    </section>

                    <section id="temporal-interpolation">
                        <h3>4.5 Temporal Interpolation with Optical Flow</h3>
                        <p>A key advantage of our optical flow-guided model, where Gaussian parameters are continuous functions of time \(t\), is the ability to perform smooth temporal interpolation. This allows rendering the scene at arbitrary frame rates higher than the original video, effectively creating slow-motion effects or denser temporal sampling at no additional training cost. The coherent trajectories enforced by optical flow ensure that these interpolated frames remain plausible.</p>
                        <figure class="side-by-side-video-pair">
                            <div class="video-row video-row-layout-2">
                                <div class="video-item">
                                    <p class="video-label">Original (1.5fps)</p>
                                    <video autoplay loop muted playsinline>
                                        <source src="videos/optflow_interp/bear_39_1p5fps.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                                <div class="video-item">
                                    <p class="video-label">Interpolated (3fps)</p>
                                    <video autoplay loop muted playsinline>
                                        <source src="videos/optflow_interp/bear_39_rendered_interpolated_iter_10000_3fps.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                </div>
                            </div>
                            <figcaption>Fig 10: Temporal interpolation. Left: Original video at 1.5fps. Right: Same scene rendered at 3fps using the learned continuous Gaussian trajectories, demonstrating smooth interpolation.</figcaption>
                        </figure>
                    </section>

                    <section id="limitations-failures">
                        <h3>4.6 Limitations & Failure Cases</h3>
                        <p>While optical flow supervision improves consistency, challenges remain, particularly for very long sequences or extremely complex motions where tracking can degrade. For instance, in a 20-frame <code>boba_20</code> clip where the cup is recolored purple, we observe that towards the end of its trajectory, some edges of the boba cup may lose their purple coloring as the initially labeled Gaussians struggle to perfectly maintain coverage and coherence over the extended motion (Fig. 11).</p>
                        <figure>
                             <video autoplay loop muted playsinline width="70%" class="vertical-video-constrained standalone-boba-video">
                                <source src="videos/limitations/boba_20_boba_recolored_to_0_0_0_0_1_0_w0_30_iter10000_optflow100000.MP4" type="video/mp4">
                            </video>
                            <figcaption>Fig 11: Example of tracking/editing degradation: recolored boba cup (purple) showing some color bleed or loss of coverage at edges after extended motion in a 20-frame clip.</figcaption>
                        </figure>
                    </section>
                </section>

                <section id="discussion">
                     <h2>5. Discussion</h2>
                    <p>Our experiments demonstrate that supervising dynamic 2D Gaussian Splatting with an optical flow consistency loss significantly enhances the temporal coherence of learned Gaussian trajectories. This directly addresses the "wave effect" observed in baseline models, where apparent motion is often a result of local Gaussian activity rather than true object transport by a consistent set of primitives. By penalizing deviations between model-implied Gaussian motion and pre-computed optical flow, Gaussians are encouraged to learn trajectories that better reflect actual pixel movements. This improved coherence translates to more robust object editing capabilities, as seen in object removal where the "void" more accurately tracks the path of the removed object, and in consistent recoloring.</p>
                <p>The "wave effect" itself is an interesting phenomenon, highlighting how reconstruction-driven optimization can find visually plausible frame-by-frame solutions that lack underlying semantic consistency. This underscores the need for explicit priors or supervisory signals that encode our understanding of how objects behave and persist over time. While our optical flow loss improves short-to-medium term tracking, the observed degradation over very long sequences suggests that maintaining global object identity purely through local frame-to-frame flow consistency is challenging. Accumulating errors or inherent limitations in current optical flow estimation for complex scenes likely contribute to this. Future work might explore integrating higher-level object tracking, more sophisticated motion models that can be regularized for long-term smoothness, or hybrid approaches combining flow with sparse semantic guidance during training.</p></section>

                <section id="conclusion">
                    <h2>6. Conclusion & Future Work</h2>
                    <p>We introduced an optical flow supervision strategy for training dynamic 2D Gaussian Splatting models, aiming to improve object-centric coherence. Our results show a marked improvement in the temporal consistency of edits like object removal and recoloring compared to a baseline model. The analysis of Gaussian motion supports the hypothesis that optical flow guidance helps transition learned trajectories from local "wave-like" phenomena to more globally consistent object transport.</p>
                <p>Key limitations include challenges with very long-term tracking and handling extremely complex object dynamics. Future work could explore: 1) Combining optical flow with sparse, strong semantic guidance from SAM-generated keyframe masks *during training* to enforce both motion and semantic identity. 2) Investigating more advanced trajectory models or object-slot attention mechanisms within the 2D Gaussian framework. 3) Developing better strategies for handling occlusions and inpainting during object removal, potentially by fine-tuning background Gaussians based on 2D inpainting results or by explicitly modeling depth layers.</p> </section>

                <section id="references">
                    <h2>7. References</h2>
                    <ol>
                        <li>Kerbl, B., Kopanas, G., Leimkühler, T., and Drettakis, G. 3d gaussian splatting for real-time radiance field rendering, 2023. URL <a href="https://arxiv.org/abs/2308.04079">https://arxiv.org/abs/2308.04079</a>.</li>
                        <li>Luiten, J., Kopanas, G., Leibe, B., and Ramanan, D. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis, 2023. URL <a href="https://arxiv.org/abs/2308.09713">https://arxiv.org/abs/2308.09713</a>.</li>
                        <li>Park, K., Sinha, U., Barron, J. T., Bouaziz, S., Goldman, D. B., Seitz, S. M., and Martin-Brualla, R. Nerfies: Deformable neural radiance fields, 2021. URL <a href="https://arxiv.org/abs/2011.12948">https://arxiv.org/abs/2011.12948</a>.</li>
                        <li>Pumarola, A., Corona, E., Pons-Moll, G., and Moreno-Noguer, F. D-nerf: Neural radiance fields for dynamic scenes, 2020. URL <a href="https://arxiv.org/abs/2011.13961">https://arxiv.org/abs/2011.13961</a>.</li>
                        <li>Zhang, X., Ge, X., Xu, T., He, D., Wang, Y., Qin, H., Lu, G., Geng, J., and Zhang, J. Gaussianimage: 1000 fps image representation and compression by 2d gaussian splatting, 2024. URL <a href="https://arxiv.org/abs/2403.08551">https://arxiv.org/abs/2403.08551</a>.</li>
                        <li>Smolak-Dyżewska, W., Malarz, D., Howil, K., Kaczmarczyk, J., Mazur, M., & Spurek, P. VeGaS: Video Gaussian Splatting, 2024. URL <a href="https://arxiv.org/abs/2411.11024">https://arxiv.org/abs/2411.11024</a>.</li>
                    </ol>
                </section>
                <footer>
                    <p>© 2025 Cheuk-Hei Chu, Margulan Ismoldayev. Final Project for 6.8300 Computer Vision, MIT.</p>
                </footer>
            </article>
        </main>
    </div>
    <script src="https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js"></script>
    <script src="script.js"></script>
</body>
</html>
