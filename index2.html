<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Flowing Gaussians: Object Editing in Dynamic 2D Scenes</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Roboto+Mono:wght@400;500&family=Source+Serif+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <script>
    MathJax = { tex: { inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']] }, svg: { fontCache: 'global' } };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css">
</head>
<body>
    <div class="page-container">
        <nav id="table-of-contents">
            <h3>Table of Contents</h3>
            <ul>
                <li><a href="#introduction">1. Introduction</a></li>
                <li><a href="#visualizing-dynamics">2. Visualizing Learned Dynamics: The "Wave"</a></li> <!-- NEW HOOK SECTION -->
                <li><a href="#background">3. Background & Motivation</a>
                    <ul>
                        <li><a href="#dynamic-2dgs">3.1 Dynamic 2D Gaussians</a></li>
                        <li><a href="#challenge-coherence">3.2 The Coherence Challenge</a></li>
                        <li><a href="#literature-review">3.3 Related Work</a></li>
                    </ul>
                </li>
                <li><a href="#our-approach">4. Our Approach: Optical Flow Supervision</a>
                    <ul>
                        <li><a href="#baseline-model-details">4.1 Baseline Model Details</a></li>
                        <li><a href="#wave-effect-analysis-detailed">4.2 Deeper Dive: The "Wave Effect"</a></li> <!-- Renamed from 3.2 -->
                        <li><a href="#optical-flow-integration">4.3 Integrating Optical Flow Loss</a></li>
                    </ul>
                </li>
                <li><a href="#experiments-results">5. Experiments & Results</a>
                    <ul>
                        <li><a href="#setup">5.1 Experimental Setup</a></li>
                        <li><a href="#reconstruction-quality">5.2 Reconstruction Quality</a></li>
                        <li><a href="#object-removal-comparison">5.3 Object Removal: Void Tracking</a></li>
                        <li><a href="#gaussian-motion-analysis">5.4 Gaussian Motion Analysis</a></li>
                        <li><a href="#editing-demos">5.5 Editing Demonstrations</a></li>
                        <li><a href="#limitations-failures">5.6 Limitations & Failure Cases</a></li>
                    </ul>
                </li>
                <li><a href="#discussion">6. Discussion</a></li>
                <li><a href="#conclusion">7. Conclusion & Future Work</a></li>
                <li><a href="#references">8. References</a></li>
            </ul>
        </nav>

        <main class="main-content-column">
            <header>
                <h1>Flowing Gaussians: Enhancing Object Coherence in Dynamic 2D Scenes via Optical Flow Supervision</h1>
                <p class="subtitle">An Investigation into Improving Editability of Dynamic 2D Gaussian Splatting Representations</p>
                <p class="authors">Your Name(s) Here</p>
                <p class="date">May 13, 2025</p>
            </header>

            <article>
                <section id="introduction">
                    <h2>1. Introduction</h2>
                    <p>Dynamic 2D Gaussian Splatting has emerged as a promising technique for representing videos, offering high-fidelity reconstruction and fast rendering. However, standard training procedures often result in representations where individual Gaussians lack long-term object-centric coherence. This fundamentally limits the ability to perform robust object-level editing tasks like removal or recoloring, as edits applied to Gaussians identified at one point in time do not consistently follow the object's perceived motion.</p>
                    <p>Our initial explorations revealed an intriguing phenomenon: the appearance of moving objects is often formed by what we term a "wave effect," where different local Gaussians activate and contribute at different times, rather than a consistent set of "object Gaussians" rigidly tracking the object. This project investigates the hypothesis that supervising the training of dynamic 2D Gaussians with dense optical flow information can enforce more coherent, object-like trajectories for the learned primitives. We demonstrate that by incorporating an optical flow consistency loss, the resulting model exhibits significantly improved temporal tracking of edited objects, allowing for more plausible object removal and recoloring. We analyze the change in learned Gaussian behavior and discuss the implications and remaining challenges of this approach.</p>
                </section>

                <section id="visualizing-dynamics"> <!-- NEW HOOK SECTION -->
                    <h2>2. Visualizing Learned Dynamics: The "Wave"</h2>
                    <p>Before diving into our proposed solution, let's visualize the intriguing, albeit problematic, behavior of Gaussians trained with only a reconstruction objective. The videos below show the animated centers of all learned Gaussians for two different scenes: a boba tea cup moving across a table ("boba_69") and a person walking up stairs ("margulan_63"). While the rendered videos (not shown here, but see Fig. 5 later) accurately reconstruct the scenes, the motion of the individual Gaussian centers reveals a fascinating pattern.</p>
                    <figure class="side-by-side-video-pair">
                        <div class="video-row video-row-layout-2">
                            <div class="video-item">
                                <p class="video-label">Boba Cup Centers (Baseline)</p>
                                <video autoplay loop muted playsinline class="vertical-video-constrained"> 
                                    <source src="videos/fallacy/boba_69_centers_iter_10000_fast.mp4" type="video/mp4">
                                </video>
                            </div>
                             <div class="video-item">
                                <p class="video-label">"Walking Up Stairs" Centers (Baseline)</p>
                                <video autoplay loop muted playsinline class="vertical-video-constrained"> 
                                    <source src="videos/fun results/margulan_63_centers_iter_10000.MP4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                        <figcaption>Fig 1: Animated Gaussian centers from baseline models (no optical flow loss). Left: "boba_69" scene. Right: "margulan_63" scene (person walking up stairs). Notice the localized, often oscillatory, motion of individual Gaussians creating a collective "wave" that forms the appearance of moving objects.</figcaption>
                    </figure>
                    <p>This "wave effect," where apparent motion arises from the sequential activation of relatively static or locally oscillating primitives rather than true transport of a coherent group, is a central challenge we aim to address for robust object-level editing.</p>
                </section>

                <section id="background">
                    <h2>3. Background & Motivation</h2> <!-- Renumbered -->
                    <section id="dynamic-2dgs">
                        <h3>3.1 Dynamic 2D Gaussian Splatting</h3>
                        <p>Our work builds upon 2D Gaussian Splatting... (Content from previous Fig 2 section)</p>
                    </section>
                    <section id="challenge-coherence">
                        <h3>3.2 The Coherence Challenge</h3> <!-- Content from previous Fig 2 section, but wave gif now Fig 3 -->
                        <p>A primary challenge ... stadium wave or the propagation of sound (Fig. 2).</p>
                        <figure>
                            <img src="videos/fallacy/Lwave-Red-2.gif" alt="Wave effect metaphor" style="max-width: 50%; height: auto; border: 1px solid var(--border-color);">
                            <figcaption>Fig 2: The "wave effect" analogy...</figcaption>
                        </figure>
                        <p>This lack of true object tracking ... demonstrated in Figure 3 using our baseline model on the <code>boba_69</code> video.</p>
                         <figure>
                            <video controls autoplay loop muted playsinline width="70%" class="vertical-video-constrained standalone-boba-video">
                                <source src="videos/fallacy/boba_69_remove_iter10000.MP4" type="video/mp4">
                            </video>
                            <figcaption>Fig 3: Object removal attempt using the baseline model ('boba_69')...</figcaption>
                        </figure>
                    </section>
                    <section id="literature-review">
                        <h3>3.3 Related Work</h3>
                        <p>Recent advancements in neural scene representations... (Content from previous Fig 2 section)</p>
                    </section>
                </section>

                <section id="our-approach">
                    <h2>4. Our Approach: Optical Flow Supervision</h2> <!-- Renumbered -->
                    <section id="baseline-model-details">
                        <h3>4.1 Baseline Model Details</h3>
                        <p>Our baseline model represents a video using \(N\) 2D Gaussian primitives... (Content from previous Fig 3 section, equations remain)</p>
                        <p>\[ \text{param}_i(t) = \sum_{j=0}^{K-1} N_{j,p}(t) \cdot \mathbf{CP}_{i,j}^{\text{param}} \]</p>
                        <p>\[ \mathcal{L}_{\text{recon}} = \sum_{t} \left( \lambda_{\text{L1}} \|\hat{I}(t) - I_{\text{gt}}(t)\|_1 + (1-\lambda_{\text{L1}})(1 - \text{MS-SSIM}(\hat{I}(t), I_{\text{gt}}(t))) \right) \]</p>
                    </section>
                    <section id="wave-effect-analysis-detailed"> <!-- Renamed from wave-effect-analysis -->
                        <h3>4.2 Deeper Dive: The "Wave Effect" in Baseline</h3> <!-- Content from previous Fig 3 section -->
                        <p>To further understand the baseline's limitations... As seen in Figure 4...</p>
                        <figure>
                            <div class="video-row video-row-layout-2">
                                <div class="video-item">
                                    <p class="video-label">Centers (Right Init, Baseline)</p>
                                    <video autoplay loop muted playsinline class="vertical-video-constrained"> 
                                        <source src="videos/fallacy/boba_69_diag_starts_right_centers.MP4" type="video/mp4">
                                    </video>
                                </div>
                                 <div class="video-item">
                                    <p class="video-label">Rendered (Right Init, Baseline)</p>
                                    <video autoplay loop muted playsinline class="vertical-video-constrained"> 
                                        <source src="videos/fallacy/boba_69_diag_starts_right_rendered.MP4" type="video/mp4">
                                    </video>
                                </div>
                            </div>
                            <figcaption>Fig 4: Diagnostic for <code>boba_69</code> baseline (Gaussians initialized on right half)...</figcaption>
                        </figure>
                    </section>
                    <section id="optical-flow-integration">
                        <h3>4.3 Integrating Optical Flow Loss</h3>
                        <p>To encourage Gaussians to learn trajectories... (Content from previous Fig 3 section, equations remain)</p>
                        <p>\[ \Delta \mathbf{xy}_{\text{model},i}(t) = xy_i(t')_{\text{model}} - xy_i(t) \]</p>
                        <p>\[ \mathcal{L}_{\text{flow}} = \sum_{t} \sum_{i \text{ s.t. } \alpha_i(t) > \tau_{\text{opac}}} \left\| \Delta \mathbf{xy}_{\text{model},i}(t) - \Delta \mathbf{xy}_{\text{gt},i}(t) \right\|^2_2 \]</p>
                        <p>\[ \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{recon}} + \lambda_{\text{flow}} \cdot \mathcal{L}_{\text{flow}} \]</p>
                    </section>
                </section>

                <section id="experiments-results">
                    <h2>5. Experiments & Results</h2> <!-- Renumbered -->
                    <section id="setup">
                        <h3>5.1 Experimental Setup</h3>
                        <p>We evaluate our approach on "boba\_69" and "bear\_39"... (Content from previous Fig 4 section)</p>
                    </section>
                    <section id="reconstruction-quality">
                        <h3>5.2 Reconstruction Quality</h3>
                        <p>Adding the optical flow loss... (Content from previous Fig 4 section)</p>
                        <figure>
                            <img src="images/psnr_table_placeholder.png" alt="PSNR/MS-SSIM Comparison - Placeholder" style="max-width: 60%; height: auto; border: 1px solid var(--border-color);">
                            <figcaption>Table 1: Reconstruction quality (PSNR dB / MS-SSIM). (Replace with actual data).</figcaption>
                        </figure>
                    </section>

                    <section id="object-removal-comparison">
                        <h3>5.3 Object Removal: Void Tracking</h3>
                        <p>A key test of object coherence is the behavior of the scene when a segmented object is removed. The baseline model often results in a "static void" (shown previously in Fig 3). With optical flow supervision, the void created by removing an object like the boba cup is expected to more accurately track the object's original path, demonstrating improved temporal consistency of the underlying Gaussian trajectories. We aim to showcase such an improved removal video if generated from a flow-supervised model.</p>
                    </section>

                    <section id="gaussian-motion-analysis">
                        <h3>5.4 Gaussian Motion Analysis</h3>
                        <p>Visualizing Gaussian center trajectories clearly illustrates the impact of optical flow...</p>
                        <figure class="side-by-side-video-pair">
                            <div class="video-row video-row-layout-2">
                                <div class="video-item"><p class="video-label">Baseline (No Flow)</p><video autoplay loop muted playsinline><source src="videos/optical_flow/bear_39_centers_iter_10000_optflow0.mp4" type="video/mp4"></video></div>
                                <div class="video-item"><p class="video-label">With Optical Flow</p><video autoplay loop muted playsinline><source src="videos/optical_flow/bear_39_centers_iter_10000_optflow10000.mp4" type="video/mp4"></video></div>
                            </div>
                            <figcaption>Fig 6: Gaussian center tracks for "bear\_39". Left: Baseline. Right: With Flow Loss.</figcaption>
                        </figure>
                        <figure class="side-by-side-video-pair">
                            <div class="video-row video-row-layout-2">
                                 <div class="video-item"><p class="video-label">Baseline (No Flow)</p><video autoplay loop muted playsinline class="vertical-video-constrained"><source src="videos/optical_flow/boba_40_centers_iter_10000_optflow0.mp4" type="video/mp4"></video></div>
                                <div class="video-item"><p class="video-label">With Optical Flow</p><video autoplay loop muted playsinline class="vertical-video-constrained"><source src="videos/optical_flow/boba_40_centers_iter_10000_optflow100000.mp4" type="video/mp4"></video></div>
                            </div>
                            <figcaption>Fig 7: Gaussian center tracks for "boba". Left: Baseline. Right: With Flow Loss.</figcaption>
                        </figure>
                    </section>
                    
                    <section id="editing-demos">
                        <h3>5.5 Editing Demonstrations with Flow-Supervised Model</h3>
                        <p>The improved object coherence enables more compelling editing applications. Here we show object recoloring on the "bear\_39" video.</p>
                         <div class="comparison-container">
                            <div class="video-wrapper">
                                <video autoplay loop muted playsinline width="100%" class="comparison-video-under" id="bearPurpleColor">
                                    <source src="videos/downstream/bear_39_bear interior_recolored_to_0_0_0_0_1_0_w0_20_iter10000_optflow30000_pts249243.MP4" type="video/mp4">
                                </video>
                                <video autoplay loop muted playsinline width="100%" class="comparison-video-over" id="bearOriginalColor">
                                    <source src="videos/downstream/bear_39.mp4" type="video/mp4">
                                </video>
                                <div class="comparison-slider" data-for-container=".comparison-container:has(#bearOriginalColor)"></div>
                            </div>
                            <figcaption>Fig 8: Interactive Comparison: Original bear (revealed left) vs. Bear recolored to purple (revealed right). Drag slider to compare.</figcaption>
                        </div>
                        <figure>
                            <p style="text-align:center;">Diagnostics for flow-trained "bear_39" model:</p>
                            <div class="video-row video-row-layout-2">
                                <div class="video-item">
                                    <p class="video-label">Segmented Bear Rendered</p><video autoplay loop muted playsinline><source src="videos/downstream/bear_39_diag_rendered_bear_interior.MP4" type="video/mp4"></video>
                                </div>
                                <div class="video-item">
                                    <p class="video-label">Segmented Bear Centers</p><video autoplay loop muted playsinline><source src="videos/downstream/bear_39_diag_centers_bear.MP4" type="video/mp4"></video>
                                </div>
                            </div>
                            <figcaption>Fig 9: Visualizing "bear interior" Gaussians from flow-trained model.</figcaption>
                        </figure>
                    </section>

                    <section id="limitations-failures">
                        <h3>5.6 Limitations & Failure Cases</h3>
                        <p>While optical flow supervision improves consistency, challenges remain, particularly for very long sequences or extremely complex motions where tracking can degrade. For instance, in a 20-frame "boba" clip where the cup is recolored purple, we observe that towards the end of its trajectory, some edges of the boba cup may lose their purple coloring as the initially labeled Gaussians struggle to perfectly maintain coverage and coherence over the extended motion (Fig. 10).</p>
                        <figure>
                             <video controls autoplay loop muted playsinline width="70%" class="vertical-video-constrained standalone-boba-video">
                                <source src="videos/limitations/boba_20_boba_recolored_to_0_0_0_0_1_0_w0_30_iter10000_optflow100000.MP4" type="video/mp4">
                            </video>
                            <figcaption>Fig 10: Example of tracking/editing degradation: recolored boba cup (purple) showing some color bleed or loss of coverage at edges after extended motion in a 20-frame clip.</figcaption>
                        </figure>
                    </section>
                </section>

                <section id="discussion">
                     <h2>6. Discussion</h2> <!-- Renumbered -->
                    <p>Our experiments demonstrate that supervising dynamic 2D Gaussian Splatting with an optical flow consistency loss significantly enhances the temporal coherence of learned Gaussian trajectories. This directly addresses the "wave effect" observed in baseline models, where apparent motion is often a result of local Gaussian activity rather than true object transport by a consistent set of primitives. By penalizing deviations between model-implied Gaussian motion and pre-computed optical flow, Gaussians are encouraged to learn trajectories that better reflect actual pixel movements. This improved coherence translates to more robust object editing capabilities, as seen in object removal where the "void" more accurately tracks the path of the removed object, and in consistent recoloring.</p>
                <p>The "wave effect" itself is an interesting phenomenon, highlighting how reconstruction-driven optimization can find visually plausible frame-by-frame solutions that lack underlying semantic consistency. This underscores the need for explicit priors or supervisory signals that encode our understanding of how objects behave and persist over time. While our optical flow loss improves short-to-medium term tracking, the observed degradation over very long sequences suggests that maintaining global object identity purely through local frame-to-frame flow consistency is challenging. Accumulating errors or inherent limitations in current optical flow estimation for complex scenes likely contribute to this. Future work might explore integrating higher-level object tracking, more sophisticated motion models that can be regularized for long-term smoothness, or hybrid approaches combining flow with sparse semantic keyframe supervision during training.</p></section>

                <section id="conclusion">
                    <h2>7. Conclusion & Future Work</h2> <!-- Renumbered -->
                    <p>We introduced an optical flow supervision strategy for training dynamic 2D Gaussian Splatting models, aiming to improve object-centric coherence. Our results show a marked improvement in the temporal consistency of edits like object removal and recoloring compared to a baseline model. The analysis of Gaussian motion supports the hypothesis that optical flow guidance helps transition learned trajectories from local "wave-like" phenomena to more globally consistent object transport.</p>
                <p>Key limitations include challenges with very long-term tracking and handling extremely complex object dynamics. Future work could explore: 1) Combining optical flow with sparse, strong semantic guidance from SAM-generated keyframe masks *during training* to enforce both motion and semantic identity. 2) Investigating more advanced trajectory models or object-slot attention mechanisms within the 2D Gaussian framework. 3) Developing better strategies for handling occlusions and inpainting during object removal, potentially by fine-tuning background Gaussians based on 2D inpainting results or by explicitly modeling depth layers.</p> </section>

                <section id="references">
                    <h2>8. References</h2> <!-- Renumbered -->
                    <p><i>(Ensure this list is comprehensive and correctly formatted.)</i></p>
                    <ol>
                        <li>Kerbl, B., et al. (2023). 3D Gaussian Splatting...</li>
                        <li>Your Base Dynamic 2D Gaussian Paper...</li>
                        <li>Kirillov, A., et al. (2023). Segment Anything...</li>
                        <li>Teed, Z., & Deng, J. (2020). RAFT...</li>
                        <li>Farnebäck, G. (2003). Two-frame motion estimation...</li>
                        <li>(Add other key papers)</li>
                    </ol>
                </section>
                <footer>
                    <p>© 2025 Your Name(s). Final Project for 6.8300 Computer Vision, MIT.</p>
                </footer>
            </article>
        </main>
    </div>
    <script src="https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js"></script>
    <script src="script.js"></script>
</body>
</html>